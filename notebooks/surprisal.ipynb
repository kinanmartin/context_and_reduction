{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       The       Ġcat        Ġis        Ġon       Ġthe       Ġmat <|endoftex <|endoftex <|endoftex <|endoftex \n",
      "     3.276      9.222      2.463      4.145      0.961      7.237        inf        inf        inf        inf \n",
      "       The       Ġcat        Ġis        Ġon       Ġthe       Ġhat <|endoftex <|endoftex <|endoftex <|endoftex \n",
      "     3.276      9.222      2.463      4.145      0.961      9.955        inf        inf        inf        inf \n",
      "       The       Ġcat        Ġis        Ġon       Ġthe     Ġpizza <|endoftex <|endoftex <|endoftex <|endoftex \n",
      "     3.276      9.222      2.463      4.145      0.961      8.212        inf        inf        inf        inf \n",
      "       The     Ġpizza        Ġis        Ġon       Ġthe       Ġmat <|endoftex <|endoftex <|endoftex <|endoftex \n",
      "     3.276     10.860      3.212      4.910      0.985      8.379        inf        inf        inf        inf \n",
      "         I      Ġtold       Ġyou      Ġthat       Ġthe       Ġcat        Ġis        Ġon       Ġthe       Ġmat \n",
      "     3.998      6.856      0.619      2.443      2.711      7.955      2.596      4.804      1.139      6.946 \n",
      "         I      Ġtold       Ġyou       Ġthe       Ġcat        Ġis        Ġon       Ġthe       Ġmat <|endoftex \n",
      "     3.998      6.856      0.619      4.115      7.612      3.031      4.817      1.233      7.033        inf \n"
     ]
    }
   ],
   "source": [
    "from surprisal import AutoHuggingFaceModel#, KenLMModel\n",
    "\n",
    "sentences = [\n",
    "    \"The cat is on the mat\",\n",
    "    \"The cat is on the hat\",\n",
    "    \"The cat is on the pizza\",\n",
    "    \"The pizza is on the mat\",\n",
    "    \"I told you that the cat is on the mat\",\n",
    "    \"I told you the cat is on the mat\",\n",
    "]\n",
    "\n",
    "m = AutoHuggingFaceModel.from_pretrained('gpt2')\n",
    "# m.to('cuda') # optionally move your model to GPU!\n",
    "\n",
    "# k = KenLMModel(model_path='./literature.arpa')\n",
    "\n",
    "for result in m.surprise(sentences):\n",
    "    print(result)\n",
    "# for result in k.surprise(sentences):\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       The       Ġcat        Ġis        Ġon       Ġthe       Ġmat <|endoftex <|endoftex <|endoftex <|endoftex \n",
      "     9.743      9.070      3.143      4.345      0.866      8.086        inf        inf        inf        inf \n",
      "       The       Ġcat        Ġis        Ġon       Ġthe       Ġhat <|endoftex <|endoftex <|endoftex <|endoftex \n",
      "     9.743      9.070      3.143      4.345      0.866      8.519        inf        inf        inf        inf \n",
      "       The       Ġcat        Ġis        Ġon       Ġthe     Ġpizza <|endoftex <|endoftex <|endoftex <|endoftex \n",
      "     9.743      9.070      3.143      4.345      0.866     10.619        inf        inf        inf        inf \n",
      "       The     Ġpizza        Ġis        Ġon       Ġthe       Ġmat <|endoftex <|endoftex <|endoftex <|endoftex \n",
      "     9.743     10.859      2.769      4.670      0.964      8.043        inf        inf        inf        inf \n",
      "         I      Ġtold       Ġyou      Ġthat       Ġthe       Ġcat        Ġis        Ġon       Ġthe       Ġmat \n",
      "     7.199      7.392      1.506      1.694      2.961      9.063      2.165      4.035      0.886      8.100 \n",
      "         I      Ġtold       Ġyou       Ġthe       Ġcat        Ġis        Ġon       Ġthe       Ġmat <|endoftex \n",
      "     7.199      7.392      1.506      3.301     10.068      2.185      4.000      0.833      8.172        inf \n"
     ]
    }
   ],
   "source": [
    "trained_model = AutoHuggingFaceModel.from_pretrained(\"../models/test2/checkpoint-37500\",\n",
    "                                                     model_class='gpt')\n",
    "# note: ensure tokenizer's settings match my pretrained models' tokenizer!\n",
    "trained_model.to('cuda') # optionally move your model to GPU!\n",
    "\n",
    "for result in trained_model.surprise(sentences):\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from CANDOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    thanks          ,      ĠTiny      Ġfirm    Ġsponge          .         ĠI     Ġswear      Ġthat         's      Ġthat         's      Ġwhat      Ġkids        Ġdo          ,      Ġthey        're      Ġjust      Ġkind        Ġof      Ġlike       Ġyou      Ġknow          ,     Ġmeant        Ġto      Ġsort        Ġof    Ġspread       Ġger         ms       Ġand   Ġviruses Ġeverywher          ,       Ġbut      Ġthey        're        Ġso      Ġcute          ,      Ġthey        Ġdo      Ġsuch         Ġa      Ġgood       Ġjob        Ġof        Ġit          .      ĠYeah          .        ĠOh \n",
      "    19.769      8.289     11.814     14.524     13.616      2.981      8.652      7.512      1.739      9.959     10.122      9.313     12.321      8.123      2.100     11.498     15.519      9.982      9.747      5.553      0.002      2.775      3.819      5.234     14.472     15.391      1.076      6.508      0.045      7.980      9.645      0.691      1.793      8.312      8.174     14.419     19.620      2.576      7.880      8.425      6.878     13.036     16.433      2.207      9.493      0.477      2.801      1.165      2.935      4.413      4.001      6.628      4.658      5.548 ]\n",
      "[    thanks          ,      ĠTiny      Ġfirm    Ġsponge          .         ĠI     Ġswear      Ġthat         's      Ġthat         's      Ġwhat      Ġkids        Ġdo          ,      Ġthey        're      Ġjust      Ġkind        Ġof      Ġlike       Ġyou      Ġknow          ,     Ġmeant        Ġto      Ġsort        Ġof    Ġspread       Ġger         ms       Ġand   Ġviruses Ġeverywher          ,       Ġbut      Ġthey        're        Ġso      Ġcute          ,      Ġthey        Ġdo      Ġsuch         Ġa      Ġgood       Ġjob        Ġof        Ġit          .      ĠYeah          .        ĠOh \n",
      "    10.819      3.757     10.579     12.847     13.601      3.259      2.661      7.568      3.530      2.710      6.685      5.622      1.550      8.779      1.254      2.644      3.006      2.679      3.141      5.658      0.030      2.840      4.034      5.627      1.056     11.363      0.451      7.561      0.145      6.855      6.165      0.020      1.941      6.166      4.693      1.969      2.235      2.159      1.118      3.486      2.890      1.936      1.859      4.659      5.742      0.229      1.525      0.022      0.797      1.946      0.692      5.886      2.469      4.730 ]\n"
     ]
    }
   ],
   "source": [
    "text = \"thanks, Tiny firm sponge. I swear that's that's what kids do, they're just kind of like you know, meant to sort of spread germs and viruses everywhere, but they're so cute, they do such a good job of it. Yeah. Oh\"\n",
    "print(trained_model.surprise(text))\n",
    "print(m.surprise(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating surprisal df for final mixed-effects analysis\n",
    "\n",
    "1. Using finetuned_model trained on a context C, calculate surprisal given context C for each word in CANDOR test set\n",
    "    - Careful of tokens to words conversion\n",
    "    - Maybe I can't do this with the surprisal library after all bc of different contexts\n",
    "2. Create df from above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\", add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  383,  3797,   318,   319,   262,  2603, 50256, 50256, 50256, 50256],\n",
      "        [  383,  3797,   318,   319,   262,  6877, 50256, 50256, 50256, 50256],\n",
      "        [  383,  3797,   318,   319,   262, 14256, 50256, 50256, 50256, 50256],\n",
      "        [  383, 14256,   318,   319,   262,  2603, 50256, 50256, 50256, 50256],\n",
      "        [  314,  1297,   345,   326,   262,  3797,   318,   319,   262,  2603],\n",
      "        [  314,  1297,   345,   262,  3797,   318,   319,   262,  2603, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokens = tokenizer(sentences, return_tensors='pt', padding=True)\n",
    "print(tokens)\n",
    "outputs = trained_model(tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "torch.Size([6, 10, 50257])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.loss)\n",
    "print(outputs.logits.size()) # this is n_sentences * max_length * vocab_size.\n",
    "# This is why i would need the logit at t_i corresponding to the actual token at t_{i+1},\n",
    "# which is what the following method will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# texts = test_dataset['text_column'].tolist()  # replace 'text_column' with the actual column name\n",
    "texts = [tokenizer.eos_token + ' ' + sentence for sentence in sentences]\n",
    "\n",
    "encoded_inputs = [tokenizer.encode(text, return_tensors=\"pt\") for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[50256,   383,  3797,   318,   319,   262,  2603]]), tensor([[50256,   383,  3797,   318,   319,   262,  6877]]), tensor([[50256,   383,  3797,   318,   319,   262, 14256]]), tensor([[50256,   383, 14256,   318,   319,   262,  2603]]), tensor([[50256,   314,  1297,   345,   326,   262,  3797,   318,   319,   262,\n",
      "          2603]]), tensor([[50256,   314,  1297,   345,   262,  3797,   318,   319,   262,  2603]])]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_surprisal(input_ids):\n",
    "    \"\"\"\n",
    "    Given a tokenized encoding of type \n",
    "        transformers.tokenization_utils_base.BatchEncoding\n",
    "\n",
    "    Return a tensor of surprisals for each token.\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_ids = input_ids.to(device)\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        logit_predictions = outputs.logits\n",
    "        \n",
    "        shift_logits = logit_predictions[..., :-1, :].contiguous()\n",
    "        shift_labels = input_ids[..., 1:].contiguous()\n",
    "        \n",
    "        loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        \n",
    "        surprisal = loss.view(shift_labels.size()).cpu().numpy().tolist()\n",
    "        \n",
    "    return surprisal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprisals = calculate_surprisal(encoded_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.689897060394287, 9.208691596984863, 2.402456283569336, 4.050849914550781, 0.8909496665000916, 7.1180925369262695]]\n"
     ]
    }
   ],
   "source": [
    "print(surprisals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.689897060394287, 9.208691596984863, 2.402456283569336, 4.050849914550781, 0.8909496665000916, 7.1180925369262695]]\n",
      "[[7.689897060394287, 9.208691596984863, 2.402456283569336, 4.050849914550781, 0.8909496665000916, 9.717320442199707]]\n",
      "[[7.689897060394287, 9.208691596984863, 2.402456283569336, 4.050849914550781, 0.8909496665000916, 8.006085395812988]]\n",
      "[[7.689897060394287, 10.804760932922363, 3.0602447986602783, 4.762859344482422, 0.9439435601234436, 8.319188117980957]]\n",
      "[[7.896890163421631, 6.833926200866699, 0.5888243317604065, 2.4493203163146973, 2.701022148132324, 7.8675336837768555, 2.5291390419006348, 4.860209941864014, 1.179627776145935, 6.884234428405762]]\n",
      "[[7.896890163421631, 6.833926200866699, 0.5888243317604065, 4.104135990142822, 7.4280242919921875, 2.951230049133301, 4.835704326629639, 1.2801216840744019, 6.991927146911621]]\n"
     ]
    }
   ],
   "source": [
    "for sentence in encoded_inputs:\n",
    "    print(calculate_surprisal(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       The       Ġcat        Ġis        Ġon       Ġthe       Ġmat <|endoftex <|endoftex <|endoftex <|endoftex \n",
      "     3.276      9.222      2.463      4.145      0.961      7.237        inf        inf        inf        inf \n",
      "       The       Ġcat        Ġis        Ġon       Ġthe       Ġhat <|endoftex <|endoftex <|endoftex <|endoftex \n",
      "     3.276      9.222      2.463      4.145      0.961      9.955        inf        inf        inf        inf \n",
      "       The       Ġcat        Ġis        Ġon       Ġthe     Ġpizza <|endoftex <|endoftex <|endoftex <|endoftex \n",
      "     3.276      9.222      2.463      4.145      0.961      8.212        inf        inf        inf        inf \n",
      "       The     Ġpizza        Ġis        Ġon       Ġthe       Ġmat <|endoftex <|endoftex <|endoftex <|endoftex \n",
      "     3.276     10.860      3.212      4.910      0.985      8.379        inf        inf        inf        inf \n",
      "         I      Ġtold       Ġyou      Ġthat       Ġthe       Ġcat        Ġis        Ġon       Ġthe       Ġmat \n",
      "     3.998      6.856      0.619      2.443      2.711      7.955      2.596      4.804      1.139      6.946 \n",
      "         I      Ġtold       Ġyou       Ġthe       Ġcat        Ġis        Ġon       Ġthe       Ġmat <|endoftex \n",
      "     3.998      6.856      0.619      4.115      7.612      3.031      4.817      1.233      7.033        inf \n"
     ]
    }
   ],
   "source": [
    "for result in m.surprise(sentences):\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above function, we have a way to go from tokens (as in pretokenized data) to surprisals per token.\n",
    "Next, we must reassign tokens to words.\n",
    "1. Based on properties of the tokens themselves (ex. begins with Ġ)\n",
    "2. using offset_mapping property from tokenizer output:\n",
    "inputs = tokenizer(\n",
    "    pretokenized_text, \n",
    "    return_offsets_mapping=True, \n",
    "    add_special_tokens=False,\n",
    "    is_split_into_words=True, \n",
    ")\n",
    "actually it's not offset_mapping, but then .word_to_tokens() method of the tokenizer output which is the easiest to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs = [tokenizer(text, return_tensors=\"pt\") for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, word in enumerate(texts[0].split()):\n",
    "#     print(f'{word=}, {encoded_inputs[0].word_to_tokens(i)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs_with_offsets = [tokenizer(\n",
    "    text.split(), \n",
    "    # return_tensors='pt',\n",
    "    return_offsets_mapping=True, \n",
    "    add_special_tokens=False,\n",
    "    is_split_into_words=True, \n",
    "    ) \n",
    "    for text in texts\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[50256,   383,  3797,   318,   319,   262,  2603]]), tensor([[50256,   383,  3797,   318,   319,   262,  6877]]), tensor([[50256,   383,  3797,   318,   319,   262, 14256]]), tensor([[50256,   383, 14256,   318,   319,   262,  2603]]), tensor([[50256,   314,  1297,   345,   326,   262,  3797,   318,   319,   262,\n",
      "          2603]]), tensor([[50256,   314,  1297,   345,   262,  3797,   318,   319,   262,  2603]])]\n",
      "[{'input_ids': [50256, 383, 3797, 318, 319, 262, 2603], 'attention_mask': [1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 13), (0, 3), (0, 3), (0, 2), (0, 2), (0, 3), (0, 3)]}, {'input_ids': [50256, 383, 3797, 318, 319, 262, 6877], 'attention_mask': [1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 13), (0, 3), (0, 3), (0, 2), (0, 2), (0, 3), (0, 3)]}, {'input_ids': [50256, 383, 3797, 318, 319, 262, 14256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 13), (0, 3), (0, 3), (0, 2), (0, 2), (0, 3), (0, 5)]}, {'input_ids': [50256, 383, 14256, 318, 319, 262, 2603], 'attention_mask': [1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 13), (0, 3), (0, 5), (0, 2), (0, 2), (0, 3), (0, 3)]}, {'input_ids': [50256, 314, 1297, 345, 326, 262, 3797, 318, 319, 262, 2603], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 13), (0, 1), (0, 4), (0, 3), (0, 4), (0, 3), (0, 3), (0, 2), (0, 2), (0, 3), (0, 3)]}, {'input_ids': [50256, 314, 1297, 345, 262, 3797, 318, 319, 262, 2603], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 13), (0, 1), (0, 4), (0, 3), (0, 3), (0, 3), (0, 2), (0, 2), (0, 3), (0, 3)]}]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_inputs)\n",
    "print(encoded_inputs_with_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [sentence for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[50256,   383,  3797,   318,   319,   262,  2603]]), tensor([[50256,   383,  3797,   318,   319,   262,  6877]]), tensor([[50256,   383,  3797,   318,   319,   262, 14256]]), tensor([[50256,   383, 14256,   318,   319,   262,  2603]]), tensor([[50256,   314,  1297,   345,   326,   262,  3797,   318,   319,   262,\n",
      "          2603]]), tensor([[50256,   314,  1297,   345,   262,  3797,   318,   319,   262,  2603]])]\n",
      "{'input_ids': [383, 3797, 318, 319, 262, 2603], 'attention_mask': [1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 3), (0, 3), (0, 2), (0, 2), (0, 3), (0, 3)]}\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "word='The', TokenSpan(start=0, end=1)\n",
      "word='cat', TokenSpan(start=1, end=2)\n",
      "word='is', TokenSpan(start=2, end=3)\n",
      "word='on', TokenSpan(start=3, end=4)\n",
      "word='the', TokenSpan(start=4, end=5)\n",
      "word='mat', TokenSpan(start=5, end=6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krmkrm/mit/meng/repos/context_and_reduction/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:361: FutureWarning: `BatchEncoding.words()` property is deprecated and should be replaced with the identical, but more self-explanatory `BatchEncoding.word_ids()` property.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text = texts[0].split()\n",
    "\n",
    "encoded_inputs_with_offsets = tokenizer(\n",
    "    text, \n",
    "    # return_tensors='pt',\n",
    "    return_offsets_mapping=True, \n",
    "    # add_special_tokens=False,\n",
    "    is_split_into_words=True, \n",
    "    ) \n",
    "    # for text in texts\n",
    "print(encoded_inputs)\n",
    "print(encoded_inputs_with_offsets)\n",
    "\n",
    "inputs = encoded_inputs_with_offsets\n",
    "print(inputs.words())\n",
    "\n",
    "for i, word in enumerate(text):\n",
    "    print(f'{word=}, {inputs.word_to_tokens(i)}') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given tokenized dataset, return surprisals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "# from transformers import GPT2TokenizerFast, GPT2Config, GPT2LMHeadModel\n",
    "# from transformers import DataCollatorForLanguageModeling\n",
    "# from transformers import Trainer, TrainingArguments\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['turn_id', 'speaker', 'start', 'stop', 'utterance', 'interval', 'delta', 'questions', 'end_question', 'overlap', 'n_words', 'input_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping'],\n",
       "        num_rows: 260\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['turn_id', 'speaker', 'start', 'stop', 'utterance', 'interval', 'delta', 'questions', 'end_question', 'overlap', 'n_words', 'input_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping'],\n",
       "        num_rows: 32\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['turn_id', 'speaker', 'start', 'stop', 'utterance', 'interval', 'delta', 'questions', 'end_question', 'overlap', 'n_words', 'input_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping'],\n",
       "        num_rows: 33\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data_path = '../data/candor/tokens_sentence/sample'\n",
    "# Load pretokenized data:\n",
    "encoded_datasets = load_from_disk(tokenized_data_path)\n",
    "encoded_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"But it was really gorgeous. It was really, really pretty and so you know, and I like to say that Iowa is pretty too, but it's namely a long kind of like the Mississippi like all those rebounds along there.\""
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_datasets['test'][2]['utterance']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
