{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config, GPT2TokenizerFast, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_tokenizer(pretrained_model_name_or_path):\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(\n",
    "        pretrained_model_name_or_path, \n",
    "        add_prefix_space=True, # ?\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token # ?\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLM:\n",
    "    \"\"\"\n",
    "    Adapted from pqian11/fragment-completion (Qian and Levy, 2022)\n",
    "    \"\"\"\n",
    "    def __init__(self, device='cuda'):\n",
    "        configuration = GPT2Config()\n",
    "        self.model = GPT2LMHeadModel(configuration).to(device)\n",
    "        self.tokenizer = load_pretrained_tokenizer('gpt2')\n",
    "        self.data_collator = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "\n",
    "        self.BLANK = '[BLANK]'\n",
    "        self.FILLER = '[FILLER]'\n",
    "        self.SEP = '[SEP]'\n",
    "        self.num_added_tokens = self.tokenizer.add_tokens([self.BLANK, self.FILLER, self.SEP])\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        self.BLANK_id = self.tokenizer.convert_tokens_to_ids(self.BLANK)\n",
    "        self.FILLER_id = self.tokenizer.convert_tokens_to_ids(self.FILLER)\n",
    "        self.SEP_id = self.tokenizer.convert_tokens_to_ids(self.SEP)\n",
    "\n",
    "    def expand_inputs(self, inputs):\n",
    "\n",
    "        input_ids = inputs['input_ids']\n",
    "        # attention_mask = features['attention_mask']\n",
    "\n",
    "        n_tokens = len(input_ids)\n",
    "\n",
    "        bidi_input_ids = [input_ids[:i] + [self.BLANK_id] + input_ids[i+1:] + [self.SEP_id, self.FILLER_id] \n",
    "                        for i in range(n_tokens)]\n",
    "\n",
    "        bidi_attention_mask = [[1 for _ in range(n_tokens + 2)] for _ in range(n_tokens)]\n",
    "\n",
    "        bidi_labels = [[-100 for _ in range(n_tokens + 1)] + [answer_token] \n",
    "                    for answer_token in input_ids]\n",
    "\n",
    "        mini_batch = {\n",
    "            'input_ids': bidi_input_ids,\n",
    "            'attention_mask': bidi_attention_mask,\n",
    "            'labels': bidi_labels\n",
    "        }\n",
    "\n",
    "        # mini_batch = {\n",
    "        #     'input_ids': torch.tensor(bidi_input_ids, dtype=torch.long),\n",
    "        #     'attention_mask': torch.tensor(bidi_attention_mask, dtype=torch.long),\n",
    "        #     'labels': torch.tensor(bidi_labels, dtype=torch.long)\n",
    "        # }\n",
    "        # print(batch)\n",
    "        return mini_batch\n",
    "    \n",
    "    def make_batch(self, mini_batches, device='cuda'):\n",
    "        \"\"\"\n",
    "        Given mini_batches (List[Dict]), create batch (Dict)\n",
    "        containing input_ids, attention_mask, and labels tensors\n",
    "\n",
    "        Reduce comprehensions for efficiency\n",
    "        \"\"\"\n",
    "        infilling_ids_batch = [batch[\"input_ids\"] for batch in mini_batches]\n",
    "        infilling_labels_batch = [batch['labels'] for batch in mini_batches]\n",
    "        batch_max_len = max(infilling_ids_batch, key=len)\n",
    "\n",
    "        infilling_ids_padded_batch = [\n",
    "            infilling_ids + [self.tokenizer.pad_token_id for _ in range(batch_max_len - len(infilling_ids))] \n",
    "            for infilling_ids in infilling_ids_batch\n",
    "        ]\n",
    "        \n",
    "        attention_mask_padded_batch = [\n",
    "            [1 for _ in range(len(infilling_ids))] + [0 for _ in range(batch_max_len - len(infilling_ids))] \n",
    "            for infilling_ids in infilling_ids_batch\n",
    "        ]\n",
    "\n",
    "        labels_padded_batch = [\n",
    "            infilling_labels + [-100 for _ in range(batch_max_len - len(infilling_labels))] \n",
    "            for infilling_labels in infilling_labels_batch\n",
    "        ]\n",
    "\n",
    "\n",
    "        batch_input_ids = torch.tensor(infilling_ids_padded_batch, dtype=torch.long, device=torch.device('cuda'))\n",
    "        batch_attention_mask = torch.tensor(attention_mask_padded_batch, dtype=torch.long, device=torch.device('cuda'))\n",
    "        batch_labels = torch.tensor(labels_padded_batch, dtype=torch.long, device=torch.device('cuda'))\n",
    "\n",
    "        batch = {\n",
    "            'input_ids': batch_input_ids,\n",
    "            'attention_mask': batch_attention_mask,\n",
    "            'labels': batch_labels\n",
    "        }\n",
    "        return batch\n",
    "    \n",
    "    def get_batch_loss(self, batch):\n",
    "        loss = self.model(batch[\"input_ids\"], labels=batch[\"label_ids\"], attention_mask=batch[\"attention_mask\"])[0]\n",
    "        # batch_token_count = np.sum([len(answer_tokens) for answer_tokens in answer_tokens_batch])\n",
    "        return loss#, batch_token_count\n",
    "\n",
    "    # def get_batch_loss(self, mini_batches, device='cuda'):\n",
    "    #     \"\"\"\n",
    "    #     Given mini_batches (List[Dict]), create batched \n",
    "    #     input_ids, attention_mask, and labels tensors and \n",
    "    #     return batch loss\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     infilling_ids_batch = [context_ids + [self.SEP_id] + answer_ids for context_ids, answer_ids in zip(context_ids_batch, answer_ids_batch)]\n",
    "    #     batch_max_len = np.max([len(infilling_ids) for infilling_ids in infilling_ids_batch])\n",
    "    #     infilling_ids_padded_batch = [infilling_ids + [self.tokenizer.bos_token_id for _ in range(batch_max_len - len(infilling_ids))] for infilling_ids in infilling_ids_batch]\n",
    "        \n",
    "    #     attention_mask = [[1 for _ in range(len(infilling_ids))] + [0 for _ in range(batch_max_len - len(infilling_ids))] for infilling_ids in infilling_ids_batch]\n",
    "    #     attention_mask = torch.tensor(attention_mask).to(device)\n",
    "\n",
    "    #     input_ids = torch.tensor(infilling_ids_padded_batch).to(device)\n",
    "    #     label_ids = [[-100 for _ in range(len(context_ids)+1)] + answer_ids + [-100 for _ in range(batch_max_len - 1 - len(context_ids) - len(answer_ids)) ] for context_ids, answer_ids in zip(context_ids_batch, answer_ids_batch)]\n",
    "    #     label_ids = torch.tensor(label_ids).to(device)\n",
    "\n",
    "    #     loss = self.model(input_ids, labels=label_ids, attention_mask=attention_mask)[0]\n",
    "    #     # batch_token_count = np.sum([len(answer_tokens) for answer_tokens in answer_tokens_batch])\n",
    "    #     return loss#, batch_token_count\n",
    "\n",
    "\n",
    "    # def get_batch_loss(self, data_batch, device='cuda'):\n",
    "    #     context_tokens_batch = [self.tokenizer.tokenize(' '.join(context)) for context, _ in data_batch]\n",
    "    #     answer_tokens_batch = [self.tokenizer.tokenize(' '.join(answer)) for _, answer in data_batch]\n",
    "\n",
    "    #     context_ids_batch = [self.tokenizer.convert_tokens_to_ids(context_tokens) for context_tokens in context_tokens_batch]\n",
    "    #     answer_ids_batch = [self.tokenizer.convert_tokens_to_ids(answer_tokens) for answer_tokens in answer_tokens_batch]\n",
    "\n",
    "    #     infilling_ids_batch = [context_ids + [self.SEP_id] + answer_ids for context_ids, answer_ids in zip(context_ids_batch, answer_ids_batch)]\n",
    "    #     batch_max_len = np.max([len(infilling_ids) for infilling_ids in infilling_ids_batch])\n",
    "    #     infilling_ids_padded_batch = [infilling_ids + [self.tokenizer.bos_token_id for _ in range(batch_max_len - len(infilling_ids))] for infilling_ids in infilling_ids_batch]\n",
    "        \n",
    "    #     attention_mask = [[1 for _ in range(len(infilling_ids))] + [0 for _ in range(batch_max_len - len(infilling_ids))] for infilling_ids in infilling_ids_batch]\n",
    "    #     attention_mask = torch.tensor(attention_mask).to(device)\n",
    "\n",
    "    #     input_ids = torch.tensor(infilling_ids_padded_batch).to(device)\n",
    "    #     label_ids = [[-100 for _ in range(len(context_ids)+1)] + answer_ids + [-100 for _ in range(batch_max_len - 1 - len(context_ids) - len(answer_ids)) ] for context_ids, answer_ids in zip(context_ids_batch, answer_ids_batch)]\n",
    "    #     label_ids = torch.tensor(label_ids).to(device)\n",
    "\n",
    "    #     loss = self.model(input_ids, labels=label_ids, attention_mask=attention_mask)[0]\n",
    "    #     batch_token_count = np.sum([len(answer_tokens) for answer_tokens in answer_tokens_batch])\n",
    "    #     return loss, batch_token_count\n",
    "    \n",
    "    # def get_loss(self, data, batch_size, device='cuda'):\n",
    "    #     total_loss = 0\n",
    "    #     total_token_count = 0\n",
    "\n",
    "    #     for data_batch in get_batches(data, batch_size):\n",
    "    #         loss, batch_token_count = self.get_batch_loss(data_batch, device=device)\n",
    "    #         total_loss += loss.item()*batch_token_count\n",
    "    #         total_token_count += batch_token_count\n",
    "\n",
    "    #     return total_loss/total_token_count\n",
    "\n",
    "    def load(self, model_path):\n",
    "        self.model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    def save(self, model_path):\n",
    "        torch.save(self.model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BidiLM = BidirectionalLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate\n",
    "import transformers\n",
    "\n",
    "transformers.__version__, accelerate.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_output_dir = '../models/test2/'\n",
    "args = TrainingArguments(\n",
    "    training_output_dir,\n",
    "    per_device_train_batch_size=128, # change to fit GPU specs\n",
    "    per_device_eval_batch_size=128,\n",
    "    group_by_length=True, # bucketing\n",
    ")\n",
    "print(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=BidiLM.model,\n",
    "    tokenizer=BidiLM.tokenizer,\n",
    "    args=args,\n",
    "    data_collator=BidiLM.data_collator,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['val'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
