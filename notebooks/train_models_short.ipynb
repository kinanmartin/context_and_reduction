{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "from transformers import GPT2TokenizerFast, GPT2Config, GPT2LMHeadModel\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56500ca919f4f6081e8139ca4324c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9360fc09c86e43689c6d28e35273efd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 470771\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 58846\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 58847\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data_in_splits(data_dir, train=0.8, val=0.1, test=0.1):\n",
    "    data = load_dataset(data_dir)\n",
    "    train_valtest = data['train'].train_test_split(test_size = 1 - train)\n",
    "    test_valid = train_valtest['test'].train_test_split(test_size = test / (val + test))\n",
    "    out = DatasetDict({\n",
    "            'train': train_valtest['train'],\n",
    "            'val': test_valid['train'],\n",
    "            'test': test_valid['test']\n",
    "        })\n",
    "    return out\n",
    "\n",
    "coca_dir = \"../data/coca_spoken/text_chunk_cleaned/\"\n",
    "\n",
    "coca_dsdict = load_data_in_splits(coca_dir, .8, .1, .1)\n",
    "coca_dsdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text']\n",
      "train {'text': 'that . SOUNDBITE OF TELEPHONE RINGING DAVID SIMON , AUTHOR , \" HOMICIDE , \" WRITER AND PRODUCER FOR \" HOMICIDE : LIFE ON THE STREET \" : My name \\'s David Simon . I wrote the book \" Homicide , \" non-fictional account that the TV show \\'s based on , and I \\'m also a writer and producer with the show @!HANSEN Why is this TV series more like an epic novel ? SIMON : Oh , I never really thought of it in those terms . I never really thought of it in epic novel terms @!HANSEN In 1988 , former \" Baltimore Sun \" reporter , David Simon , wrote the book , \" Homicide : A Year on the Killing Streets . \" Movie director Barry Levinson , screenwriter Paul Attanasio , and television producer Tom Fontana adapted his real-life stories of Baltimore detectives to the small screen @!SIMON I think one of the things that appeals to me about the show is it \\'s almost a demythification of police in the sense that all the cop shows prior , in some sense , have been'}\n",
      "val {'text': 'bring it under the scope of the state . Once upon a time- SAM DONALDSON : Well , now- COKIE ROBERTS : I mean , I really do think- SAM DONALDSON : Hold on , folks . Let- you have n\\'t even heard this liberal behaviorist argument . At age 15 , I guess we would argue that , no , it would be inappropriate behavior . You pick the age - 18 ? COKIE ROBERTS : Yeah . SAM DONALDSON : All right , 12 ? Now , yes . At six and seven , this little boy meant no harm , but he somehow- DAVID BRINKLEY : Does the arrival of puberty have something to do with this ? SAM DONALDSON : -ought to be told - and that \\'s why I say the punishment is the question . No , he should not be disciplined in some severe way , but he should be- he- GEORGE WILL : What happened to the principal calling the mother and- COKIE ROBERTS : Right , exactly . SAM DONALDSON : He should- exactly . He should be told that , \"'}\n",
      "test {'text': '. @!JOHN-MERROW : Thanks to President Obama , Arne Duncan has the opportunity to become the most powerful U.S. secretary of education ever . @!ARNE-DUNCAN-Secre : This was not something I aspired to do . Frankly , were it anyone but him , I wouldnt probably do it . @!JOHN-MERROW : The two developed a close relationship in Chicago , both on and off the court . @!ARNE-DUNCAN- : I took him to good schools ; I took him to tough schools . So weve had a chance to work together for years . @!JOHN-MERROW : That bond , plus an unprecedented level of federal education spending , means Duncan could have a real impact on a troubled public system where nationally 3 out of 10 ninth-graders fail to graduate . @!ARNE-DUNCAN- : Theres a huge opportunity in the stimulus package to reward great behavior and to get folks thinking differently about how we best serve children . My simple rule is , if its good for children , were going to do more of it ; if its not good for children , were going to do less . @!JOHN-MERROW'}\n"
     ]
    }
   ],
   "source": [
    "print(coca_dsdict['train'].column_names)\n",
    "for split in ['train', 'val', 'test']:\n",
    "    print(split, random.choice(coca_dsdict[split]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize (or load tokenized data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "context_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50257\n",
      "Max Model Input Sizes: 1024\n",
      "Padding token: None\n",
      "Special tokens: ['<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size:\", tokenizer.vocab_size)\n",
    "print(\"Max Model Input Sizes:\", tokenizer.model_max_length)\n",
    "print(\"Padding token:\", tokenizer.pad_token)\n",
    "print(\"Special tokens:\", tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_path = '../data/coca_spoken/tokens_chunk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ada6dcb13b4c36a9fb0b520069abc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/470771 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8adac8fc6a491e9feaea2c7a58ee4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef727afa8d0455d8bb9b8e26949e01a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58847 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb679add2f94699806db1bfbc367902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/470771 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7d2507020245d4915907dc82dee12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/58846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eadd8b5ba024f4fab4c0e9d2ac8b004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/58847 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize:\n",
    "encoded_datasets = coca_dsdict.map(\n",
    "    lambda chunk: tokenizer(\n",
    "        chunk['text'],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "        ), \n",
    "    batched=True)\n",
    "encoded_datasets.save_to_disk(tokenized_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretokenized data:\n",
    "encoded_datasets = load_from_disk(tokenized_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping'],\n",
      "        num_rows: 470771\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping'],\n",
      "        num_rows: 58846\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping'],\n",
      "        num_rows: 58847\n",
      "    })\n",
      "})\n",
      "{'text': 'risk worth taking . So , I think , in two years , let everybodys taxes go up , and so we can have some sort of fiscal future . @!MARK-SHIELDS : Who are we kidding -- two years , let them go up ? They were going to go up after 10 years . Thats -- the Bush tax cuts were supposed to expire , have to expire , by law , at 10 years . All right ? Now , were on the cusp by every indication -- David agrees ... @!JIM-LEHRER : Sort of . He sort of agrees . @!MARK-SHIELDS : ... of a Republican tsunami -- no , a Republican tsunami in November . And were going to count in two years that this is going to be right at John Boehners -- Speaker John Boehner and Majority Leader Mitch McConnell -- this is going to be their galvanizing idea , is to repeal the tax cuts , as described . They will extend them in perpetuity . I mean , there is one organizing principle among all the Republicans , whether they are Tea Parties', 'input_ids': [19121, 2861, 2263, 764, 1406, 837, 314, 892, 837, 287, 734, 812, 837, 1309, 790, 65, 375, 893, 5704, 467, 510, 837, 290, 523, 356, 460, 423, 617, 3297, 286, 9068, 2003, 764, 2488, 0, 44, 14175, 12, 9693, 40, 3698, 5258, 1058, 5338, 389, 356, 26471, 1377, 734, 812, 837, 1309, 606, 467, 510, 5633, 1119, 547, 1016, 284, 467, 510, 706, 838, 812, 764, 1320, 82, 1377, 262, 5511, 1687, 6630, 547, 4385, 284, 24264, 837, 423, 284, 24264, 837, 416, 1099, 837, 379, 838, 812, 764, 1439, 826, 5633, 2735, 837, 547, 319, 262, 269, 17723, 416, 790, 12955, 1377, 3271, 14386, 2644, 2488, 0, 41, 3955, 12, 2538, 17184, 1137, 1058, 33947, 286, 764, 679, 3297, 286, 14386, 764, 2488, 0, 44, 14175, 12, 9693, 40, 3698, 5258, 1058, 2644, 286, 257, 3415, 31019, 1377, 645, 837, 257, 3415, 31019, 287, 3389, 764, 843, 547, 1016, 284, 954, 287, 734, 812, 326, 428, 318, 1016, 284, 307, 826, 379, 1757, 12415, 71, 2741, 1377, 14931, 1757, 26273, 290, 22171, 10540, 20472, 18184, 1377, 428, 318, 1016, 284, 307, 511, 42170, 2890, 2126, 837, 318, 284, 14634, 262, 1687, 6630, 837, 355, 3417, 764, 1119, 481, 9117, 606, 287, 8939, 14834, 764, 314, 1612, 837, 612, 318, 530, 16924, 7989, 1871, 477, 262, 4734, 837, 1771, 484, 389, 15777, 32024], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'length': 223, 'overflow_to_sample_mapping': 0}\n"
     ]
    }
   ],
   "source": [
    "print(encoded_datasets)\n",
    "print(encoded_datasets['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping'],\n",
       "        num_rows: 470771\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping'],\n",
       "        num_rows: 58846\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping'],\n",
       "        num_rows: 58847\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = encoded_datasets.remove_columns(['text'])\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = GPT2Config()\n",
    "model = GPT2LMHeadModel(configuration)\n",
    "configuration = model.config\n",
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token # why?\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data_collation = data_collator([tokenized_datasets['train'][i] for i in range(40)])\n",
    "for key in example_data_collation:\n",
    "    print(f\"{key} shape: {example_data_collation[key].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate\n",
    "import transformers\n",
    "\n",
    "transformers.__version__, accelerate.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_output_dir = '../models/test2/'\n",
    "args = TrainingArguments(\n",
    "    training_output_dir,\n",
    "    per_device_train_batch_size=128, # change to fit GPU specs\n",
    "    per_device_eval_batch_size=128,\n",
    "    group_by_length=True, # bucketing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['val'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to resume\n",
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Did you know that the first person to\"\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = GPT2LMHeadModel.from_pretrained(\"../models/test2/checkpoint-37500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = trained_model.generate(inputs, max_length=50, num_return_sequences=1)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = trained_model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    # temperature=0.6,\n",
    ")\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "generated_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
