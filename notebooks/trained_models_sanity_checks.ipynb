{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "scripts_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'scripts'))\n",
    "\n",
    "if scripts_dir not in sys.path:\n",
    "    sys.path.append(scripts_dir)\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "candor_convo_path = '../data/candor/sample/0020a0c5-1658-4747-99c1-2839e736b481/'\n",
    "\n",
    "models = {\n",
    "    ('left', 'sentence'): '../models/gpt2/left_sentence/checkpoint-76066',\n",
    "    ('right', 'sentence'): '../models/gpt2/right_sentence/checkpoint-76066',\n",
    "    ('bidi', 'sentence'): '../models/gpt2/bidi_sentence/checkpoint-76066',\n",
    "    ('left', 'bigram'): '../models/gpt2/left_bigram/checkpoint-63819',\n",
    "    ('right', 'bigram'): '../models/gpt2/right_bigram/checkpoint-63819',\n",
    "    ('bidi', 'bigram'): '../models/gpt2/bidi_bigram/checkpoint-100000',\n",
    "}\n",
    "\n",
    "context_direction, context_size = 'left', 'sentence'\n",
    "model_dir = models[(context_direction, context_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model from ../models/gpt2/left_sentence/checkpoint-76066...\n",
      "...done\n",
      "\n",
      "Loading pretrained tokenizer from gpt2...\n",
      "Vocabulary size: 50257\n",
      "Max Model Input Sizes: 1024\n",
      "Special tokens: ['[BOS]', '[EOS]', '<|endoftext|>']\n",
      "...done\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candor_convo_path = Path(candor_convo_path)\n",
    "\n",
    "model = load_pretrained_model(model_dir)\n",
    "tokenizer = load_pretrained_tokenizer(\n",
    "    'gpt2', \n",
    "    context_size=context_size, \n",
    "    context_direction=context_direction, \n",
    "    add_prefix_space=True\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[50257,   383,  1266,  1517]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = ['[BOS]'] + \"The best thing\".split(' ')\n",
    "inputs = tokenizer(input_text, is_split_into_words=True, return_tensors='pt')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50257,   383,  1266,  1517,   546,   340,   318,   326,   262,   661,\n",
      "           508,   389,   287,   262,  2422,   389,   407,  1016,   284,   307,\n",
      "          1498,   284,   466,   326,    13,   366,   290,   366,   383,    12,\n",
      "            54,   861, 16288,   366,   318,  1016,   284,   307,   257,   845,\n",
      "           922,   530,    13,   366,   290,   366,   383,   968,  1971,  3782]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[BOS] The best thing about it is that the people who are in the military are not going to be able to do that. \" and \" The-Wertheimer \" is going to be a very good one. \" and \" The New York Times'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(inputs['input_ids'], max_length=50, num_return_sequences=1)\n",
    "print(outputs)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The best thing I can do to help ourselves, the most difficult thing to do is to get out there and get an investigation into the case. \" I\\'m, I\\'ve been looking for some time with any evidence'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    # temperature=0.6,\n",
    ")\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "generated_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "car1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
