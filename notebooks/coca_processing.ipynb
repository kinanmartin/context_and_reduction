{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "coca_dir = \"../data/coca/text/text_spoken_kde/\"\n",
    "\n",
    "# dataset = load_dataset('text', data_dir=coca_dir)\n",
    "dataset = load_dataset('text', data_files=coca_dir+'w_spok_201*.txt')\n",
    "train_dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##4072661 @!NANCY-GRACE-HOST : A 5-year-old Florida girl tucked into bed , five hours later , shes g\n"
     ]
    }
   ],
   "source": [
    "example_line = random.choice(train_dataset)\n",
    "print(example_line['text'][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing (text cleaning)\n",
    "\n",
    "Goal: From COCA's spoken genre, make a .txt file of new-line separated sentences. Clean formatting incl. speaker codes and weird tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_string_id=81\n",
      "len(example_string)=15290\n",
      "##4072612 @!ROBIN-ROBERTS-@1-A# @(Off-camera) Oh , come on upstairs here , Sam . Now , the dos and d\n"
     ]
    }
   ],
   "source": [
    "example_string_id = random.randint(0, len(train_dataset) - 1)\n",
    "example_string = train_dataset[example_string_id]['text']\n",
    "print(f'{example_string_id=}')\n",
    "print(f'{len(example_string)=}')\n",
    "print(example_string[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 [929, 972, 1084, 1047, 1018, 982, 1020, 929, 1053, 1054, 943, 1122, 1023, 1053, 767]\n",
      "##4072612 @!ROBIN-ROBERTS-@1-A# @(Off-camera) Oh ,\n",
      ". @!STEVE-HARVEY-@1-AB# @(Off-camera) Yeah . @!ROB\n",
      "couple . We have , let 's see , Robert is a 51-yea\n",
      "lets Christine know they 're having a wonderful da\n",
      "right there . Just meeting somebody . @GRAPHICS @G\n",
      "whole evening right there . @!ROBIN-ROBERTS-@1-A# \n",
      "audience are here all shaking , you , you like tha\n",
      "to know before you waste a lot of time , emotion ,\n",
      "that lady keep talking about anyway . @!ROBIN-ROBE\n",
      "@!DEE-DEE-@130'S-PUB# There are better products no\n",
      "right . @!ROBIN-ROBERTS-@1-A# @(Off-camera) It was\n",
      "noticed he was a little ... @!ROBIN-ROBERTS-@1-A# \n",
      ", you know , I do n't like your hair . What are yo\n",
      "you know , they are going out on a second date . O\n",
      "skating . @!GEORGE-STEPHANOPOU# @(Off-camera) Also\n"
     ]
    }
   ],
   "source": [
    "def separate_chunks(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    COCA is composed of scrambled chunks split by \"@\" * 10 (possibly \n",
    "    cut off at end of file). \n",
    "    Returns a list of separated chunks.\n",
    "    \"\"\"\n",
    "    return text.split(' @ @ @ @ @ @ @ @ @ @ ')\n",
    "\n",
    "\n",
    "example_chunks = separate_chunks(example_string)\n",
    "print(len(example_chunks), [len(chunk) for chunk in example_chunks])\n",
    "for chunk in example_chunks:\n",
    "    print(chunk[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_chunk_id=0\n",
      "##4072612 @!ROBIN-ROBERTS-@1-A# @(Off-camera) Oh , come on upstairs here , Sam . Now , the dos and don'ts of a first date . What can you do on a first date to make sure there is a second one ? A third one ? We found some brave souls who let us follow them on a blind date to find out what they 're doing right , and what they could be doing better . Joining us now with advice for our singles is \" GMA 's \" relationship guru , and the author of the best-selling relationship book , \" Act Like A Lady , Think Like A Man , \" Mr. Steve Harvey . And you always come with your own audience too here . @GRAPHICS @GRAPHICS @!STEVE-HARVEY-@1-AB# @(Off-camera) I bring a crowd with me . I always do better with people watching me . I do n't know what , a little showoff thing , maybe , I do n't know . @!ROBIN-ROBERTS-@1-A# @(Off-camera) There 's something , get on with that , but I 'm , I 'm going to let that go . @!STEVE-HARVEY-@1-AB#\n",
      "##4072612 .  .  Oh , come on upstairs here , Sam . Now , the dos and don'ts of a first date . What can you do on a first date to make sure there is a second one ? A third one ? We found some brave souls who let us follow them on a blind date to find out what they 're doing right , and what they could be doing better . Joining us now with advice for our singles is \" GMA 's \" relationship guru , and the author of the best-selling relationship book , \" Act Like A Lady , Think Like A Man , \" Mr. Steve Harvey . And you always come with your own audience too here . .  .  .  .  I bring a crowd with me . I always do better with people watching me . I do n't know what , a little showoff thing , maybe , I do n't know . .  .  There 's something , get on with that , but I 'm , I 'm going to let that go . . \n",
      "----\n",
      "0 ##4072612 \n",
      "1 Oh , come on upstairs here , Sam . Now , the dos and don'ts of a first date . What can you do on a first date to make sure there is a second one ? A third one ? We found some brave souls who let us follow them on a blind date to find out what they 're doing right , and what they could be doing better . Joining us now with advice for our singles is \" GMA 's \" relationship guru , and the author of the best-selling relationship book , \" Act Like A Lady , Think Like A Man , \" Mr. Steve Harvey . And you always come with your own audience too here . \n",
      "2 I bring a crowd with me . I always do better with people watching me . I do n't know what , a little showoff thing , maybe , I do n't know . \n",
      "3 There 's something , get on with that , but I 'm , I 'm going to let that go . @!STEVE-HARVEY-@1-AB#\n"
     ]
    }
   ],
   "source": [
    "def remove_speaker_and_other_tags(chunk: str, remove_nonspeaker_tags=True) -> str:\n",
    "    \"\"\"\n",
    "    DEPRECATED: it's better to split text by these tags instead of removing them\n",
    "    Remove from one chunk speaker tags (ex: @!BOB:) and optionally\n",
    "    other tags (ex: @(End-of-clip)).\n",
    "    \"\"\"\n",
    "    pattern = r\"\\s+@\\S+\" if remove_nonspeaker_tags else r\"\\s+@!\\S+\"\n",
    "    return re.sub(pattern, \" . \", chunk)\n",
    "\n",
    "def split_by_speaker_and_other_tags(\n",
    "        chunk: str, \n",
    "        remove_nonspeaker_tags=True,\n",
    "        ) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits one chunk by speaker tags (ex: @!BOB) and optionally\n",
    "        other tags (ex: @(End-of-clip)).\n",
    "\n",
    "    remove_nonspeaker_tags: also removes things like @(End-of-clip). \n",
    "        speaker tag: @!BOB  non-speaker tag @BOB (no \"!\")\n",
    "        Does not remove long portions inside of @(Clip-from-previous blocks\n",
    "    \n",
    "    Notes:\n",
    "        - Pattern makes first word in turn start with a space.\n",
    "        To remove it, add an \\s at the end of the pattern, but be aware\n",
    "        that this will break pattern matching of consecutive tags.\n",
    "        - Speaker tags are inconsistently either marked as \n",
    "            \"@!BOB\", \"@!BOB:\", \"@!BOB :\", \"@!BOB ( voiceover ) :\", \n",
    "            and more. ( voiceover ) is currently not captured.\n",
    "\n",
    "    \"\"\"\n",
    "    # pattern = r\"\\s+@\\S+\" if remove_nonspeaker_tags else r\"\\s+@!\\S+\"\n",
    "    pattern = r\"@\\S+(?:\\s:|)\\s\" if remove_nonspeaker_tags else r\"@!\\S+(?:\\s:|)\\s\"\n",
    "    out = re.split(pattern, chunk)\n",
    "    out = [segment for segment in out if segment.strip()]\n",
    "    return out\n",
    "\n",
    "\n",
    "example_chunk_id = random.randint(0, len(example_chunks)-1)\n",
    "print(f'{example_chunk_id=}')\n",
    "example_chunk = example_chunks[example_chunk_id]\n",
    "print(example_chunk)\n",
    "print(remove_speaker_and_other_tags(example_chunk))\n",
    "print('----')\n",
    "example_turns = split_by_speaker_and_other_tags(example_chunk)\n",
    "for turn_number, turn in enumerate(example_turns):\n",
    "    print(turn_number, turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Oh , come on upstairs here , Sam .',\n",
       " \"Now , the dos and don'ts of a first date .\",\n",
       " 'What can you do on a first date to make sure there is a second one ?',\n",
       " 'A third one ?',\n",
       " \"We found some brave souls who let us follow them on a blind date to find out what they 're doing right , and what they could be doing better .\",\n",
       " 'Joining us now with advice for our singles is \" GMA \\'s \" relationship guru , and the author of the best-selling relationship book , \" Act Like A Lady , Think Like A Man , \" Mr. Steve Harvey .',\n",
       " 'And you always come with your own audience too here .']"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_turn_into_sentences(\n",
    "        turn: str, \n",
    "        # exclude_sentences_with_ellipses=False\n",
    "        ) -> str:\n",
    "    \"\"\"\n",
    "    Splits one tag-free turn (as separated by split_by_speaker_and_other_tags) \n",
    "        into sentences.\n",
    "    Since COCA has space-separated punctuation, splits are done by:\n",
    "        [' . ', ' ? ', ' ! ']\n",
    "    \"\"\"\n",
    "    delimiters = [' . ', ' ? ', ' ! ']\n",
    "    pattern = \"|\".join(map(re.escape, delimiters))\n",
    "    pattern = '(' + pattern + ')' # retain delimiters\n",
    "    splits = re.split(pattern, turn)\n",
    "    if len(splits) == 1:\n",
    "        return splits\n",
    "    \n",
    "    # For multi-sentence utterances, we must manually re-combine punctuation\n",
    "    out = []\n",
    "    for idx, split in enumerate(splits):\n",
    "        if not split:\n",
    "            continue\n",
    "        if not (idx % 2): # is sentence\n",
    "            out.append(split)\n",
    "        else: # is delimiter\n",
    "            out[-1] += split[:-1] # don't include space after punctuation\n",
    "    return out\n",
    "    \n",
    "turn = example_turns[random.randint(0, len(example_turns)-1)]\n",
    "split_turn_into_sentences(turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_number=3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"I 'm glad that we got together today to have this wonderful date together .\",\n",
       " 'Yes .',\n",
       " \"It 's really a pleasure to , to have this nice date with you .\",\n",
       " 'I enjoy talking to you .',\n",
       " \"I mean , I think , I think , you 're , you 're actually , you could be a friend of mine .\",\n",
       " 'Really .',\n",
       " 'I really like you .',\n",
       " \"You 're very sweet .\",\n",
       " 'Very nice person .',\n",
       " 'I want to say , I like Robert .',\n",
       " 'I did too , I like him .',\n",
       " \"I like how he 's all like this , and he 's really relaxed .\",\n",
       " 'See , he had that little swagger. ',\n",
       " 'I know , he did .',\n",
       " 'He slumped in his seat .',\n",
       " 'He was handling it .',\n",
       " 'Little leery when he kept his legs crossed the entire time .',\n",
       " \"I do n't know what was going on .\",\n",
       " 'But he started with a hug .']"
      ]
     },
     "execution_count": 691,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_chunk_into_sentences(\n",
    "        chunk: str,\n",
    "        exclude_first_and_last_sentences=True,\n",
    "        remove_nonspeaker_tags=True,\n",
    "        ) -> List[str]:\n",
    "    \"\"\"\n",
    "    Combines `split_by_speaker_and_other_tags` and \n",
    "        `split_turn_into_sentences` to split a COCA chunk\n",
    "        into a list of sentences.\n",
    "\n",
    "    exclude_first_and_last_sentences: because the first and \n",
    "        last sentences are likely fragments split by the chunk border\n",
    "    \"\"\"\n",
    "    turns = split_by_speaker_and_other_tags(chunk, \n",
    "                                            remove_nonspeaker_tags)\n",
    "    sentences = []\n",
    "    for turn in turns:\n",
    "        sentences.extend(split_turn_into_sentences(turn))\n",
    "    return sentences[1:-1] if exclude_first_and_last_sentences else sentences\n",
    "\n",
    "chunk_number = random.randint(0, len(example_chunks)-1)\n",
    "example_chunk = example_chunks[chunk_number]\n",
    "example_sentences = split_chunk_into_sentences(example_chunk, \n",
    "                                               exclude_first_and_last_sentences=True)\n",
    "print(f'{chunk_number=}')\n",
    "example_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lets Christine know they 're having a wonderful date . @!ROBERT-@151-YEAR-O# I 'm glad that we got together today to have this wonderful date together . @!CHRISTINE-@146-YEA# Yes . @!ROBERT-@151-YEAR-O# It 's really a pleasure to , to have this nice date with you . I enjoy talking to you . I mean , I think , I think , you 're , you 're actually , you could be a friend of mine . Really . I really like you . You 're very sweet . Very nice person . @!ROBIN-ROBERTS-@1-A# @(Off-camera) I want to say , I like Robert . @!STEVE-HARVEY-@1-AB# @(Off-camera) I did too , I like him . @!ROBIN-ROBERTS-@1-A# @(Off-camera) I like how he 's all like this , and he 's really relaxed . @!STEVE-HARVEY-@1-AB# @(Off-camera) See , he had that little swagger. @!ROBIN-ROBERTS-@1-A# @(Off-camera) I know , he did . @!STEVE-HARVEY-@1-AB# @(Off-camera) He slumped in his seat . He was handling it . Little leery when he kept his legs crossed the entire time . I do n't know what was going on . @!ROBIN-ROBERTS-@1-A# @(Off-camera) But he started with a hug . And you\n"
     ]
    }
   ],
   "source": [
    "print(example_chunks[chunk_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3025/3025 [00:00<00:00, 5782.66it/s]\n"
     ]
    }
   ],
   "source": [
    "def clean_coca_file(\n",
    "        input_file_path: Path,\n",
    "        output_dir_path: Path,\n",
    "        overwrite=True,\n",
    "        exclude_first_and_last_sentences=True,\n",
    "        remove_nonspeaker_tags=True,\n",
    "        ) -> None:\n",
    "    assert input_file_path.exists(), f'File \"{input_file_path}\" not found'\n",
    "    dataset_dict = load_dataset('text', data_files=str(input_file_path))\n",
    "    dataset = dataset_dict['train']\n",
    "\n",
    "    output_dir_path.mkdir(parents=True, exist_ok=overwrite)\n",
    "    output_file_path = output_dir_path / (input_file_path.stem + '_cleaned.txt')\n",
    "\n",
    "    f = open(output_file_path, 'w')\n",
    "    for line in tqdm(dataset):\n",
    "        text = line['text']\n",
    "        chunks = separate_chunks(text)\n",
    "        for chunk in chunks:\n",
    "            sentences = split_chunk_into_sentences(chunk,\n",
    "                                                   exclude_first_and_last_sentences,\n",
    "                                                   remove_nonspeaker_tags)\n",
    "            f.write('\\n'.join(sentences) + '\\n')\n",
    "\n",
    "    f.close()\n",
    "    return None\n",
    "        \n",
    "clean_coca_file(\n",
    "    input_file_path=Path(\"../data/coca/text/text_spoken_kde/w_spok_2000.txt\"),\n",
    "    output_dir_path=Path(\"../data/coca_spoken/text_cleaned/\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 2399 examples [00:00, 7528.44 examples/s]\n",
      "100%|██████████| 2399/2399 [00:00<00:00, 6725.94it/s]\n",
      "100%|██████████| 3025/3025 [00:00<00:00, 6972.86it/s]\n",
      "Generating train split: 2771 examples [00:00, 18821.67 examples/s]\n",
      "100%|██████████| 2771/2771 [00:00<00:00, 6742.26it/s]\n",
      "Generating train split: 3079 examples [00:00, 22134.69 examples/s]\n",
      "100%|██████████| 3079/3079 [00:00<00:00, 7683.01it/s]\n",
      "Generating train split: 1913 examples [00:00, 15499.17 examples/s]\n",
      "100%|██████████| 1913/1913 [00:00<00:00, 4472.69it/s]\n",
      "Generating train split: 563 examples [00:00, 7163.92 examples/s]\n",
      "100%|██████████| 563/563 [00:00<00:00, 2252.76it/s]\n",
      "Generating train split: 2324 examples [00:00, 21070.03 examples/s]\n",
      "100%|██████████| 2324/2324 [00:00<00:00, 5892.23it/s]\n",
      "Generating train split: 972 examples [00:00, 8249.32 examples/s]\n",
      "100%|██████████| 972/972 [00:00<00:00, 2186.51it/s]\n",
      "Generating train split: 2647 examples [00:00, 21737.80 examples/s]\n",
      "100%|██████████| 2647/2647 [00:00<00:00, 7172.12it/s]\n",
      "Generating train split: 2132 examples [00:00, 17439.08 examples/s]\n",
      "100%|██████████| 2132/2132 [00:00<00:00, 5834.05it/s]\n",
      "Generating train split: 1006 examples [00:00, 7801.84 examples/s]\n",
      "100%|██████████| 1006/1006 [00:00<00:00, 2265.24it/s]\n",
      "Generating train split: 1783 examples [00:00, 15649.54 examples/s]\n",
      "100%|██████████| 1783/1783 [00:00<00:00, 4279.49it/s]\n",
      "Generating train split: 1670 examples [00:00, 14616.04 examples/s]\n",
      "100%|██████████| 1670/1670 [00:00<00:00, 4167.46it/s]\n",
      "Generating train split: 760 examples [00:00, 6901.18 examples/s]\n",
      "100%|██████████| 760/760 [00:00<00:00, 2595.94it/s]\n",
      "Generating train split: 720 examples [00:00, 6991.25 examples/s]\n",
      "100%|██████████| 720/720 [00:00<00:00, 2449.85it/s]\n",
      "Generating train split: 1472 examples [00:00, 13276.68 examples/s]\n",
      "100%|██████████| 1472/1472 [00:00<00:00, 4061.70it/s]\n",
      "Generating train split: 1426 examples [00:00, 11822.86 examples/s]\n",
      "100%|██████████| 1426/1426 [00:00<00:00, 4204.21it/s]\n",
      "Generating train split: 1296 examples [00:00, 12795.34 examples/s]\n",
      "100%|██████████| 1296/1296 [00:00<00:00, 3991.17it/s]\n",
      "Generating train split: 1452 examples [00:00, 15143.10 examples/s]\n",
      "100%|██████████| 1452/1452 [00:00<00:00, 3601.12it/s]\n",
      "Generating train split: 1482 examples [00:00, 13697.51 examples/s]\n",
      "100%|██████████| 1482/1482 [00:00<00:00, 4362.46it/s]\n",
      "Generating train split: 1552 examples [00:00, 13894.50 examples/s]\n",
      "100%|██████████| 1552/1552 [00:00<00:00, 4356.33it/s]\n",
      "Generating train split: 916 examples [00:00, 10689.72 examples/s]\n",
      "100%|██████████| 916/916 [00:00<00:00, 2500.59it/s]\n",
      "Generating train split: 826 examples [00:00, 8406.09 examples/s]\n",
      "100%|██████████| 826/826 [00:00<00:00, 2011.31it/s]\n"
     ]
    }
   ],
   "source": [
    "for file in Path(\"../data/coca/text/text_spoken_kde/\").iterdir():\n",
    "    clean_coca_file(file, Path(\"../data/coca_spoken/text_cleaned/\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
