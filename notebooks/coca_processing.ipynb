{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "coca_dir = \"../data/coca/text/text_spoken_kde/\"\n",
    "\n",
    "# dataset = load_dataset('text', data_dir=coca_dir)\n",
    "dataset = load_dataset('text', data_files=coca_dir+'w_spok_201*.txt')\n",
    "train_dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##4103113 @!ROBERT-JEFFRESS-@1# We want a candidate who is a good , moral person , or do we want a c\n"
     ]
    }
   ],
   "source": [
    "example_line = random.choice(train_dataset)\n",
    "print(example_line['text'][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing (text cleaning)\n",
    "\n",
    "Goal: From COCA's spoken genre, make a .txt file of new-line separated sentences. Clean formatting incl. speaker codes and weird tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_string_id=1809\n",
      "len(example_string)=11366\n",
      "##4103331 @!ROBIN-ROBERTS-@1-A# @(Off-camera) Now to the latest on Gary Giordano. @!ROBIN-ROBERTS-@1\n"
     ]
    }
   ],
   "source": [
    "example_string_id = random.randint(0, len(train_dataset) - 1)\n",
    "example_string = train_dataset[example_string_id]['text']\n",
    "print(f'{example_string_id=}')\n",
    "print(f'{len(example_string)=}')\n",
    "print(example_string[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 [1178, 1204, 1152, 1016, 1015, 1052, 946, 893, 1115, 1145, 440]\n",
      "##4103331 @!ROBIN-ROBERTS-@1-A# @(Off-camera) Now \n",
      "@GRAPHICS @!DAN-HARRIS-@1-ABC-# @(Voiceover) And r\n",
      ", behind me . And she wasn't. @!DAN-HARRIS-@1-ABC-\n",
      "before somebody is satisfied . I do miss her . And\n",
      "exactly what happened . Okay . @!DAN-ABRAMS-@1-ABC\n",
      "well , that , that he 's thinking about the attorn\n",
      "those are some of the questions you asked . You kn\n",
      "he 's , he 's talked about it so often that that c\n",
      "you have gone through interrogation , and you get \n",
      "taken . @!ROBIN-ROBERTS-@1-A# @(Off-camera) ... yo\n",
      "not offering specific details that I would think a\n"
     ]
    }
   ],
   "source": [
    "def separate_chunks(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    COCA is composed of scrambled chunks split by \"@\" * 10 (possibly \n",
    "    cut off at end of file). \n",
    "    Returns a list of separated chunks.\n",
    "    \"\"\"\n",
    "    return text.split(' @ @ @ @ @ @ @ @ @ @ ')\n",
    "\n",
    "\n",
    "example_chunks = separate_chunks(example_string)\n",
    "print(len(example_chunks), [len(chunk) for chunk in example_chunks])\n",
    "for chunk in example_chunks:\n",
    "    print(chunk[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_chunk_id=4\n",
      "exactly what happened . Okay . @!DAN-ABRAMS-@1-ABC-# @(Voiceover) But to go on and then be so belligerent , and to antagonize often the people who are doing the interviews does n't help him at all . And so I 'm not certain that in the end this was a net win for Giordano. @!ROBIN-ROBERTS-@1-A# @(Off-camera) But you think , Michael , that there is a reason , because many people watching both interviews were saying there 's no emotion from him when it comes to Robyn . And just when he kept saying - I 've answered this 50 , 60 times , I do n't wan na do it again . You say there 's a reason for that . @!DOCTOR-MICHAEL-WEL# @(Off-camera) I feel , I feel a little bit more careful about interpreting what I 'm seeing in an interview when you have an attorney right next to someone . It 's rehearsed . Your interview was rehearsed . Another interview was rehearsed . And to whatever degree an attorney can control his client , he 'll even allow that interview to go forward . @!DOCTOR-MICHAEL-WEL# @(Voiceover) So I\n",
      "exactly what happened . Okay . .  .  But to go on and then be so belligerent , and to antagonize often the people who are doing the interviews does n't help him at all . And so I 'm not certain that in the end this was a net win for Giordano. .  .  But you think , Michael , that there is a reason , because many people watching both interviews were saying there 's no emotion from him when it comes to Robyn . And just when he kept saying - I 've answered this 50 , 60 times , I do n't wan na do it again . You say there 's a reason for that . .  .  I feel , I feel a little bit more careful about interpreting what I 'm seeing in an interview when you have an attorney right next to someone . It 's rehearsed . Your interview was rehearsed . Another interview was rehearsed . And to whatever degree an attorney can control his client , he 'll even allow that interview to go forward . .  .  So I\n",
      "----\n",
      "0 exactly what happened . Okay . \n",
      "1 But to go on and then be so belligerent , and to antagonize often the people who are doing the interviews does n't help him at all . And so I 'm not certain that in the end this was a net win for Giordano. \n",
      "2 But you think , Michael , that there is a reason , because many people watching both interviews were saying there 's no emotion from him when it comes to Robyn . And just when he kept saying - I 've answered this 50 , 60 times , I do n't wan na do it again . You say there 's a reason for that . \n",
      "3 I feel , I feel a little bit more careful about interpreting what I 'm seeing in an interview when you have an attorney right next to someone . It 's rehearsed . Your interview was rehearsed . Another interview was rehearsed . And to whatever degree an attorney can control his client , he 'll even allow that interview to go forward . \n",
      "4 So I\n"
     ]
    }
   ],
   "source": [
    "def remove_speaker_and_other_tags(chunk: str, remove_nonspeaker_tags=True) -> str:\n",
    "    \"\"\"\n",
    "    DEPRECATED: it's better to split text by these tags instead of removing them\n",
    "    Remove from one chunk speaker tags (ex: @!BOB:) and optionally\n",
    "    other tags (ex: @(End-of-clip)).\n",
    "    \"\"\"\n",
    "    pattern = r\"\\s+@\\S+\" if remove_nonspeaker_tags else r\"\\s+@!\\S+\"\n",
    "    return re.sub(pattern, \" . \", chunk)\n",
    "\n",
    "def split_by_speaker_and_other_tags(\n",
    "        chunk: str, \n",
    "        remove_nonspeaker_tags=True,\n",
    "        ) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits one chunk by speaker tags (ex: @!BOB) and optionally\n",
    "        other tags (ex: @(End-of-clip)).\n",
    "\n",
    "    remove_nonspeaker_tags: also removes things like @(End-of-clip). \n",
    "        speaker tag: @!BOB  non-speaker tag @BOB (no \"!\")\n",
    "        Does not remove long portions inside of @(Clip-from-previous blocks\n",
    "    \n",
    "    Notes:\n",
    "        - Pattern makes first word in turn start with a space.\n",
    "        To remove it, add an \\s at the end of the pattern, but be aware\n",
    "        that this will break pattern matching of consecutive tags.\n",
    "        - Speaker tags are inconsistently either marked as \n",
    "            \"@!BOB\", \"@!BOB:\", \"@!BOB :\", \"@!BOB ( voiceover ) :\", \n",
    "            and more. ( voiceover ) is currently not captured.\n",
    "\n",
    "    \"\"\"\n",
    "    # pattern = r\"\\s+@\\S+\" if remove_nonspeaker_tags else r\"\\s+@!\\S+\"\n",
    "    pattern = r\"@\\S+(?:\\s:|)\\s\" if remove_nonspeaker_tags else r\"@!\\S+(?:\\s:|)\\s\"\n",
    "    out = re.split(pattern, chunk)\n",
    "    out = [segment for segment in out if segment.strip()]\n",
    "    return out\n",
    "\n",
    "\n",
    "example_chunk_id = random.randint(0, len(example_chunks)-1)\n",
    "print(f'{example_chunk_id=}')\n",
    "example_chunk = example_chunks[example_chunk_id]\n",
    "print(example_chunk)\n",
    "print(remove_speaker_and_other_tags(example_chunk))\n",
    "print('----')\n",
    "example_turns = split_by_speaker_and_other_tags(example_chunk)\n",
    "for turn_number, turn in enumerate(example_turns):\n",
    "    print(turn_number, turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"But you think , Michael , that there is a reason , because many people watching both interviews were saying there 's no emotion from him when it comes to Robyn .\",\n",
       " \"And just when he kept saying - I 've answered this 50 , 60 times , I do n't wan na do it again .\",\n",
       " \"You say there 's a reason for that .\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_turn_into_sentences(\n",
    "        turn: str, \n",
    "        # exclude_sentences_with_ellipses=False\n",
    "        ) -> str:\n",
    "    \"\"\"\n",
    "    Splits one tag-free turn (as separated by split_by_speaker_and_other_tags) \n",
    "        into sentences.\n",
    "    Since COCA has space-separated punctuation, splits are done by:\n",
    "        [' . ', ' ? ', ' ! ']\n",
    "    \"\"\"\n",
    "    delimiters = [' . ', ' ? ', ' ! ']\n",
    "    pattern = \"|\".join(map(re.escape, delimiters))\n",
    "    pattern = '(' + pattern + ')' # retain delimiters\n",
    "    splits = re.split(pattern, turn)\n",
    "    if len(splits) == 1:\n",
    "        return splits\n",
    "    \n",
    "    # For multi-sentence utterances, we must manually re-combine punctuation\n",
    "    out = []\n",
    "    for idx, split in enumerate(splits):\n",
    "        if not split:\n",
    "            continue\n",
    "        if not (idx % 2): # is sentence\n",
    "            out.append(split)\n",
    "        else: # is delimiter\n",
    "            out[-1] += split[:-1] # don't include space after punctuation\n",
    "    return out\n",
    "    \n",
    "turn = example_turns[random.randint(0, len(example_turns)-1)]\n",
    "split_turn_into_sentences(turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_number=5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"I do n't think people in America recognize what it 's like to be sitting here , talking and recognizing how many people are listening to you and then have your freedom jeopardized at the same time , recognizing that you have so much hanging on every word .\",\n",
       " 'So I think the , the pressure ... ',\n",
       " 'Mm-hmm .',\n",
       " \"... operating within it does affect someone 's expression .\",\n",
       " \"But that 's , but that 's the attorney 's job .\",\n",
       " \"I mean , the attorney 's job ... \",\n",
       " \"That 's right .\",\n",
       " \"That 's right .\",\n",
       " 'I agree .',\n",
       " 'Sure .',\n",
       " \"... should be to make sure , and , and he was totally unrehearsed , I mean , in my view , meaning , I think Giordano should have spent more time thinking about his answers and how he 's gon na respond .\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_chunk_into_sentences(\n",
    "        chunk: str,\n",
    "        exclude_first_and_last_sentences=True,\n",
    "        remove_nonspeaker_tags=True,\n",
    "        ) -> List[str]:\n",
    "    \"\"\"\n",
    "    Combines `split_by_speaker_and_other_tags` and \n",
    "        `split_turn_into_sentences` to split a COCA chunk\n",
    "        into a list of sentences.\n",
    "\n",
    "    exclude_first_and_last_sentences: because the first and \n",
    "        last sentences are likely fragments split by the chunk border\n",
    "    \"\"\"\n",
    "    turns = split_by_speaker_and_other_tags(chunk, \n",
    "                                            remove_nonspeaker_tags)\n",
    "    sentences = []\n",
    "    for turn in turns:\n",
    "        sentences.extend(split_turn_into_sentences(turn))\n",
    "    return sentences[1:-1] if exclude_first_and_last_sentences else sentences\n",
    "\n",
    "chunk_number = random.randint(0, len(example_chunks)-1)\n",
    "example_chunk = example_chunks[chunk_number]\n",
    "example_sentences = split_chunk_into_sentences(example_chunk, \n",
    "                                               exclude_first_and_last_sentences=True)\n",
    "print(f'{chunk_number=}')\n",
    "example_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well , that , that he 's thinking about the attorney next to him . I do n't think people in America recognize what it 's like to be sitting here , talking and recognizing how many people are listening to you and then have your freedom jeopardized at the same time , recognizing that you have so much hanging on every word . @!DOCTOR-MICHAEL-WEL# @(Off-camera) So I think the , the pressure ... @!ROBIN-ROBERTS-@1-A# @(Off-camera) Mm-hmm . @!DOCTOR-MICHAEL-WEL# @(Off-camera) ... operating within it does affect someone 's expression . @!DAN-ABRAMS-@1-ABC-# @(Off-camera) But that 's , but that 's the attorney 's job . I mean , the attorney 's job ... @!DOCTOR-MICHAEL-WEL# @(Off-camera) That 's right . That 's right . I agree . Sure . @!DAN-ABRAMS-@1-ABC-# @(Off-camera) ... should be to make sure , and , and he was totally unrehearsed , I mean , in my view , meaning , I think Giordano should have spent more time thinking about his answers and how he 's gon na respond . And it seem to me either his lawyer , Jose Baez , did n't prepare him at all\n"
     ]
    }
   ],
   "source": [
    "print(example_chunks[chunk_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_coca_file(\n",
    "        input_file_path: Path,\n",
    "        output_dir_path: Path,\n",
    "        split_by='chunk', # 'sentence'\n",
    "        overwrite=True,\n",
    "        exclude_first_and_last_sentences=True,\n",
    "        remove_nonspeaker_tags=True,\n",
    "        ) -> None:\n",
    "    assert input_file_path.exists(), f'File \"{input_file_path}\" not found'\n",
    "    assert split_by in ['chunk', 'sentence'], f'Invalid split method: choose from [\"chunk\", \"sentence\"]'\n",
    "    dataset_dict = load_dataset('text', data_files=str(input_file_path))\n",
    "    dataset = dataset_dict['train']\n",
    "\n",
    "    output_dir_path.mkdir(parents=True, exist_ok=overwrite)\n",
    "    output_file_path = output_dir_path / (input_file_path.stem + '_cleaned.txt')\n",
    "\n",
    "    f = open(output_file_path, 'w')\n",
    "    for line in tqdm(dataset):\n",
    "        text = line['text']\n",
    "        chunks = separate_chunks(text)\n",
    "        if split_by == 'chunk':\n",
    "            f.write('\\n'.join(chunks) + '\\n')\n",
    "        elif split_by == 'sentence':\n",
    "            for chunk in chunks:\n",
    "                sentences = split_chunk_into_sentences(chunk,\n",
    "                                                    exclude_first_and_last_sentences,\n",
    "                                                    remove_nonspeaker_tags)\n",
    "                f.write('\\n'.join(sentences) + '\\n')\n",
    "\n",
    "    f.close()\n",
    "    return None\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3025/3025 [00:00<00:00, 24878.86it/s]\n"
     ]
    }
   ],
   "source": [
    "clean_coca_file(\n",
    "    input_file_path=Path(\"../data/coca/text/text_spoken_kde/w_spok_2000.txt\"),\n",
    "    output_dir_path=Path(\"../data/coca_spoken/text_chunk_cleaned/\"),\n",
    "    split_by='chunk'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2399/2399 [00:00<00:00, 19427.00it/s]\n",
      "100%|██████████| 3025/3025 [00:00<00:00, 28911.07it/s]\n",
      "100%|██████████| 2771/2771 [00:00<00:00, 25081.07it/s]\n",
      "100%|██████████| 3079/3079 [00:00<00:00, 25170.03it/s]\n",
      "100%|██████████| 1913/1913 [00:00<00:00, 18782.34it/s]\n",
      "100%|██████████| 563/563 [00:00<00:00, 10866.87it/s]\n",
      "100%|██████████| 2324/2324 [00:00<00:00, 21597.32it/s]\n",
      "100%|██████████| 972/972 [00:00<00:00, 10217.81it/s]\n",
      "100%|██████████| 2647/2647 [00:00<00:00, 23244.61it/s]\n",
      "100%|██████████| 2132/2132 [00:00<00:00, 20658.49it/s]\n",
      "100%|██████████| 1006/1006 [00:00<00:00, 10582.01it/s]\n",
      "100%|██████████| 1783/1783 [00:00<00:00, 5893.63it/s]\n",
      "100%|██████████| 1670/1670 [00:00<00:00, 16039.33it/s]\n",
      "100%|██████████| 760/760 [00:00<00:00, 7800.72it/s]\n",
      "100%|██████████| 720/720 [00:00<00:00, 7330.01it/s]\n",
      "100%|██████████| 1472/1472 [00:00<00:00, 15450.53it/s]\n",
      "100%|██████████| 1426/1426 [00:00<00:00, 11973.65it/s]\n",
      "100%|██████████| 1296/1296 [00:00<00:00, 9772.13it/s]\n",
      "100%|██████████| 1452/1452 [00:00<00:00, 15886.23it/s]\n",
      "100%|██████████| 1482/1482 [00:00<00:00, 13505.38it/s]\n",
      "100%|██████████| 1552/1552 [00:00<00:00, 13126.84it/s]\n",
      "100%|██████████| 916/916 [00:00<00:00, 8842.49it/s]\n",
      "100%|██████████| 826/826 [00:00<00:00, 8716.30it/s]\n"
     ]
    }
   ],
   "source": [
    "for file in Path(\"../data/coca/text/text_spoken_kde/\").iterdir():\n",
    "    clean_coca_file(\n",
    "        input_file_path=file, \n",
    "        output_dir_path=Path(\"../data/coca_spoken/text_chunk_cleaned/\"),\n",
    "        split_by='chunk'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
